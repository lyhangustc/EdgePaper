\section{Introduction}
% emphasize the chanllenges of edge2face!
Realistic image synthesis has been a hot topic in computer vision and computer graphics for years. 
Traditional methods \cite{EBSR, TextureSyn,SceneCompletion} establish databases of existing images, and generate images by matching and fusing images in the database patch-wisely. 
With the emergence of deep neural networks (DNN), several promising DNN-based approaches for image synthesis have been proposed. 
Variational autoencoders (VAEs) \cite{VAEs}, which maximize a variational lower bound on the log-likelihood of the training data, have brought some progress in generating visually plausible imagesl. However, the generated samples suffer from being blurry. 
\xj{The PixelCNN decoders~\cite{PixelCNN} generate convincing images using autoregressive models. However, the pixel-wise sampling procedure make this model computationally expensive. }
%

\xj{Among these DNN-based methods}, generative adversarial networks (GANs)~\cite{GANs} offer a new and promising mechanism to generate images.
GANs take noise vectors as input, and train two networks playing a minmax game to guide the generated samples to be indistinguishable from the real ones. 
Conditional GANs, which generate image from assigned conditional information instead of noise vector, are conditional versions of GANs. 
Conditional GANs are trained in a supervised manner and shown to be powerful in modeling the conditional distributions with respect to the assigned conditions. 
A variety of conditions have been applied to conditional GANs, such as discrete class labels \cite{cGANs}, texts \cite{StackGANs, StackGANs++}, and images.
%
\xj{As a specific application of CGANs,} image-to-image translation has drawn a lot of attention recently.
It aims to apply a conditional image in one domain to generate the corresponding target image in another, while preserving shared concepts, objects or scenes in these two images. 
Since the first image-to-image model (pix2pix)~\cite{pix2pix} was proposed, there have been many variants of this approach in both supervised and unsupervised manner \cite{CycleGANs, DualGANs,CoupleGANs,BicycleGANs}. % Why not compare to the other method
\comments{
However, the pix2pix model has troubles in some cases of translating face images from the corresponding edge maps. 
For example, this model occasionally fails to synthesize faces with the whole set of well-defined structures, e.g. eyes, noses, mouths, and etc., especially when the conditional edge map lack of one or several parts of the structure. 
}
\xj{These models successfully synthesize realistic textures when fine structures are given in the conditional image, or inferring the global structure in a specfific object class. When the geometric structure is partially provided with different level of details, which is exactly the case of line drawings or edge maps, these models fails to complete the missing structure while preserving the given structure details.} 

The underlying reason is mainly two-fold.
%
First, the existing GANs are mainly built based on convolutional layers. Since the convolution operator has a local receptive field depending on the size of its kernels, a large receptive field is achieved by cooperation of several convolution layers. It is hard for the network optimizer to discover proper parameter values that model the long-range dependencies through several convolutional layers~\cite{SAGANs}.  
Secondly, the existing discriminator used in GANs focuses on examining local patches instead of capturing the global information, and therefore fails to guide the generator to synthesize the global structure of the conditional image. 


Considering the first reason, we introduce a conditional self-attention mechanism to the generator of image-to-image models to address the problem.
Self-attention \cite{Non-local, Attention, MachineReading, SAGANs}, which computes the response at a position as a weighted sum of the features at all positions, is able to capture the long-range dependencies across different regions of images and feature maps. In order to adapting the conditional setting of image-to-image translation and encouraging the model to leverage the information of the conditional image directly, we propose a conditional self-attention module (CSAM) which enables the higher layers to sense the conditional image. 
%
For the second reason, we consider to establish multiple discriminators to capture information of different levels, both patch-wisely and globally. We note that similar idea of multiple discriminators has been raised by \cite{LaplaceGANs, SGANs, StackGANs, CRN} who resizes the real/fake samples and applies multiple discriminators to these multi-scale samples. 
\red{(xuejin: so what is the novelty of our method compared to them?? )}

\comments{
In this research, we propose Conditional Self-attention Generative Adversarial Networks (SCGANs), which translate images from one domain to another being able to capture long-range dependencies and reserve the global structures. With the help of the novel CSAMs, the conditional image is able to guide the higher layers in the architecture directly.  
}



On the whole, our contributions are summarized as follow:
%
\begin{enumerate}
	\item We firstly introduce the self-attention mechanism to image-to-image translation and propose a novel conditional self-attention generative adversarial networks for the image-to-image translation task. Unlike convolution-based methods, the proposed model is able to model the long-range dependencies and global structure across images.
	\item We propose a multi-level discriminator to the image-to-image translation. The proposed discriminator is able to capture the global structure information as well as the local realism. 
	\item We show the effectiveness of the proposed model by experiments. Two kinds of user studies are investigated to show the perceptual evaluation of the results generated by the proposed method. \red{Quantitative evaluation is conducted by calculating the FID of the pix2pix model and the proposed model.}
	
\end{enumerate}

 

The rest of this article is organized as follow. Related work is presented in Section \ref{sec:related_work}. The method we proposed is introduced in Section \ref{sec:method}. We demonstrate the effectiveness of method by a series of experiments in Section \ref{sec:experiment}. Section \ref{sec:conclusion} summarizes our conclusions and outlines 
possible future work.