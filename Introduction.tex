\section{Introduction}
% emphasize the chanllenges of edge2face!
Realistic image synthesis has been a hot topic in computer vision and computer graphics for years. Traditional non-parametric methods \cite{see DCGAN paper} oftern matching images patch-wisely with a database of existing images. With the emergence of deep neural networks (DNN), several promising DNN-bsed approaches for image synthesis have been proposed. Variational autoencoders (VAEs) \cite{VAEs}, which maximize a variational lower bound on the log-likelihood of the training data, have brought some progress in generating visually plausible images, but the generated samples suffer from being blurry. Autoregressive models \cite{PixCNN} directly model the conditional distributions over pixels. Though generating convincing samples, these models are costly to sample from.
%

Generative adversarial networks (GANs) \cite{GANs} offer a new and promising mechanism to generate images, which take noises vectors as input and train two networks playing minmax game to guide the generated samples to be indistinguishable from the real ones. 
Conditional GANs are generalized versions of GANs in a conditional setting. Instead of noise vectors, conditional GANs are able to be applied to generate images with a variety of conditions, such as discrete class labels \cite{cGANs}, texts \cite{StackGANs, StackGANs++}, and lower-resolution images \cite{SRGANs, more SR}. Conditional GANs are trained in a supervised manner and shown to be able to model the conditional distribution with respect to the assigned condition.
%

%
Among these conditional image generation methods, image-to-image translation has drawn a lot of attention recently, which aims to apply a source image in one domain to generate a corresponding target image in another, reserving shared concepts, objects or scenes in these two images. Since \cite{pix2pix}  raised the first image-to-image model (pix2pix), there have been many variants of this approach in both supervised and unsupervised manner \cite{CycleGANs, DualGANs,CoupleGANs,BicycleGANs}. However, image-to-image translation models based on convolutional neural networks may have troubles to generate some classes of realistic images, especially when these images have structural constrains. For example, when generating faces from corresponding edge maps (edge-to-face), convolutional-based pix2pix model sometimes fails to xxxxx >>>>add a figure to explain<<<<. The reasons behind this might be two-folded. 1)
Since the convolution operator has a local receptive field depending on the size of its kernels, a large receptive field is achieved by cooperation of several convolution layers. It is hard for the optimizer to discover parameter values that models the long-range dependencies through several convolutional layers \cite{SAGANs}. 2) The discriminator used in pix2pix models \cite{PatchGANs} focuses on examining local patches instead of capturing the global information, and therefore fails to guide the generator to synthesize the global structure of the conditional image. 

Considering the first reason, we introduce a conditional self-attention mechanism to the generator of image-to-image models to address the problem.
Self-attention \cite{Non-local, Attention, MachineReading, SAGAN}, which computes the response at a position as a weighted sum of the features at all positions, is able to capture the long-range dependencies across different regions of images and feature maps. In order to adapting the conditional setting of image-to-image translation and encouraging the model to leverage the information of conditional image directly, we propose a conditional self-attention module (CSM) which enables the higher layers to sense the conditional image. 
%
For the second reason, we consider to establish the multiple discriminators to capture information of different level, both patch-wisely and globally. We note that similar idea of multiple discriminators is raised by \cite{Multi-D} who resizes the real/fake samples and applies multiple discriminators to these multi-scale samples. 

In this research, we propose Conditional Self-attention Generative Adversarial Networks (SCGANs), which translate images from one domain to another being able to capture long-range dependencies and reserve the global structures. With the help with the novel CSMs, the conditional image are able to guide the higher layers of the model directly.  

Our contributions are summarized as follow:

i) We propose a novel conditional self-attention generative adversarial networks for the image-to-image translation task, which are able to model the long-range dependencies and global structure across images.

ii)

iii)

The rest of this article is organized as follow. Related works are presented in Section \ref{sec:2}. The method we proposed is introduced in Section \ref{sec:3}. We show the experiments to prove the effectiveness of method in Section \ref{sec:4}