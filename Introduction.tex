\section{Introduction}
% emphasize the chanllenges of edge2face!
Realistic image synthesis has been a hot topic in computer vision and computer graphics for years. 
Traditional methods \cite{EBSR, TextureSyn,SceneCompletion} establish databases of existing images, and generate images by matching and merging images in the database patch-wisely. 
With the emergence of deep neural networks (DNN), several promising DNN-bsed approaches for image synthesis have been proposed. 
Variational autoencoders (VAEs) \cite{VAEs}, which maximize a variational lower bound on the log-likelihood of the training data, have brought some progress in generating visually plausible images, but the generated samples suffer from being blurry. 
Autoregressive models \cite{PixelCNN} generate image pixel by pixel and are able to generate convincing samples. However, it is computational inefficient to sample images from these models.
%

Generative adversarial networks (GANs) \cite{GANs} offer a new and promising mechanism to generate images, which take noise vectors as input and train two networks playing minmax game to guide the generated samples to be indistinguishable from the real ones. 
Conditional GANs, which generate image from assigned conditional information instead of noise vector, are conditional versions of GANs. Conditional GANs are trained in a supervised manner and shown to be powerful in modeling the conditional distributions with respect to the assigned conditions. A variety of conditions have been applied to conditional GANs, such as discrete class labels \cite{cGANs}, texts \cite{StackGANs, StackGANs++}, and images.
%
Among these conditional image generation methods, image-to-image translation has drawn a lot of attention recently, which aims to apply a conditional image in one domain to generate the corresponding target image in another, reserving shared concepts, objects or scenes in these two images. Since the first image-to-image model (pix2pix)  \cite{pix2pix} was proposed, there have been many variants of this approach in both supervised and unsupervised manner \cite{CycleGANs, DualGANs,CoupleGANs,BicycleGANs}. % Why not compare to the other method
However, the pix2pix model has troubles in some cases of translating face images from the corresponding edge maps. For example, this model occasionally fails to synthesize faces with the whole set of well-defined structures, e.g. eyes, noses, mouths, and etc., especially when the conditional edge map lack of one or several parts of the structure. 

The reasons behind this might be two-folded.
%
1) The pix2pix model is built based on convolution layers. Since the convolution operator has a local receptive field depending on the size of its kernels, a large receptive field is achieved by cooperation of several convolution layers. It is hard for the optimizer to discover parameter values that model the long-range dependencies through several convolutional layers \cite{SAGANs}. 
2) The discriminator used in the pix2pix model \cite{pix2pix} focuses on examining local patches instead of capturing the global information, and therefore fails to guide the generator to synthesize the global structure of the conditional image. 

Considering the first reason, we introduce a conditional self-attention mechanism to the generator of image-to-image models to address the problem.
Self-attention \cite{Non-local, Attention, MachineReading, SAGANs}, which computes the response at a position as a weighted sum of the features at all positions, is able to capture the long-range dependencies across different regions of images and feature maps. In order to adapting the conditional setting of image-to-image translation and encouraging the model to leverage the information of the conditional image directly, we propose a conditional self-attention module (CSAM) which enables the higher layers to sense the conditional image. 
%
For the second reason, we consider to establish multiple discriminators to capture information of different levels, both patch-wisely and globally. We note that similar idea of multiple discriminators has been raised by \cite{LaplaceGANs, SGANs, StackGANs, CRN} who resizes the real/fake samples and applies multiple discriminators to these multi-scale samples. 

In this research, we propose Conditional Self-attention Generative Adversarial Networks (SCGANs), which translate images from one domain to another being able to capture long-range dependencies and reserve the global structures. With the help of the novel CSAMs, the conditional image is able to guide the higher layers in the architecture directly.  

Our contributions are summarized as follow:

i) We firstly introduce the self-attention mechanism to image-to-image translation and propose a novel conditional self-attention generative adversarial networks for the image-to-image translation task. Unlike convolutional-based methods, the proposed model is able to model the long-range dependencies and global structure across images.

ii) We propose a multi-level discriminator to the image-to-image translation. The proposed discriminator is able to capture the global structure information as well as the local realism. 

iii) We show the effectiveness of the proposed model by experiments. Two kinds of user studies are investigated to show the perceptual evaluation of the results generated by the proposed method. Quantitative evaluation is conducted by calculating the FID of the pix2pix model and the proposed model.

The rest of this article is organized as follow. Related works are presented in Section \ref{sec:related_work}. The method we proposed is introduced in Section \ref{sec:method}. We demonstrate the effectiveness of method by experiments in Section \ref{sec:experiment}. Section \ref{sec:conclusion} summarizes our conclusions and outlines 
possible future work.