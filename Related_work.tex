\section{Related work}
\label{sec:related_work}
Our work is based on image-to-image translation frameworks, which are variants of GANs in a conditional setting. In this section, we present related research in GANs, conditional GANs, and image-to-image translation models. We also give a brief review on recently proposed attention models.
\subsection{Generative Adversarial Networks (GANs)}
Generative adversarial networks (GANs) \cite{GANs} have obtained a great success in recent years.
Based on the minmax game theory, a classical architecture of GANs contains a generator network and a discriminator network. The generator is takes a noise vector as input and generates samples indistinguishable from the real ones, while the discriminator, in opposite, attempt to find out whether its input is real or synthesized. The minmax game played by these two networks guides the generated distribution to be similar to the real data distribution. Compared to other deep  framework of image generation \cite{VAEs, PixelCNN}, GANs are able to synthesize images with less blurriness and provide a more efficient process to generate samples. However, GANs suffer from several problems in the early stage, such as the instability of training and the mode collapse problem. To stabilize the training of GANs and enable GANs to generate images with high quality and large diversity, many efforts have been made. Deep convolutional GANs (DCGANs) \cite{DCGANs} first introduced a convolutional architecture which led to visually improved quality. \cite{Improved_Techniques} proposed an approach to train discriminator in a semi-supervised fashion, granting the discriminator's internal representations knowledge of the class structure of (some fraction of) the training data it is presented. Energy based GANs (EBGANs) \cite{EBGANs}  were proposed as a class of GANs that aimed to model the discriminator as an energy function. This variant converges more stably and is not only easy to train but also robust to hyper-parameter variations. Wasserstein distance, which acts as a loss as well as a measure of convergence in training process, is brought to GANs by \cite{WGANs, WGAN-GP} to benefit both the stability and mode coverage. Several other works \cite{LSGANs, BEGANs, DRAGANs} also make progress in stabilizing the training and increasing the diversity of the results of GANs. A recently proposed work \cite{SAGANs} introduced self-attention mechanism to the unconditional GANs and achieved state-of-the-art results. Our work is based on GANs in a conditional setting.
%

\subsection{Conditional Generative Adversarial Networks}
Conditional GANs are generalized versions of GANs in a conditional setting. Instead of taking a noise vector as input, conditional GANs generates images based on the assigned conditions and models the conditional distributions of the samples with respect to the conditions. 
Conditional GANs were firstly introduced by \cite{cGANs} who treated the conditional generation problem as the inverse processing of image classification and used discrete labels as condition to generate images. Previous works have explored GANs generating images based on a wide variety of conditions.
\cite{InfoGANs, ACGANs} took both noise vectors and discrete class label as input and added a classifier task to the discriminator in two different architectures to generate images with high recognizability.   
%
\cite{3DGANs} trained convolutional networks to generate images of objects given object style, viewpoint and color. With the experiments of interpolating viewpoints, they showed that networks learn a meaningful representation of 3D models. 
%
\cite{StackGANs, StackGANs++} generated high-resolution photo-realistic images conditioned on text descriptions in two stages, where GANs sketched the basic shapes and colors in the first stage and added details to the generated images in the second stage.  
%
A recently proposed method\cite{cGAN-projection} leveraged the conditional information in a novel way, where the discriminator involves an inner product term between the condition vector and the feature vector in a middle level layer. This formulation is based on the observation that the loss function of GANs are able to be decomposed into the sum of two log likelihood ratios.
%
Our work utilize GANs in a conditional setting to generate images from images, which utilize the condition information directly even in the high-level layers.
%
%
\subsection{Image-to-image translation with GANs}
% TODO: modify something like [1] studied xxx
Given an image in one domain, image-to-image translation methods generate a corresponding image in another domain. These two images are possible representations of the same scene or object. Image-to-image translation with GANs is a special case of conditional GANs where images are applied to be conditions. 
%

The pix2pix method \cite{pix2pix} firstly introduced the concept of image-to-image translation. Pix2pix is trained in a supervised manner, where the training dataset is a set of paired images. 
%Unlike prior works which dealt with only one application, \cite{pix2pix} are able to handle multiple applications in one frame work only switching training datasets. 
Pix2pix applies skip connections \cite{Unet} between mirrored layers in the generator to make sure low-level information pass through its encoder-decoder architecture and uses patch discriminators to increase the performance of the generator. However, the convolution-based architecture makes it difficult to discover the long-range dependencies across the images and feature maps, and the patch-wise discriminator is not able to ensure the global structure information to be well captured by the model.
%
In addition to pix2pix, many image-to-image tasks are trained in a supervised manner. 
\cite{CRN, pix2pixHD} used coarse-to-fine refinement frameworks to synthesis photographic images from semantic label maps. 
\cite{outdoor_scene} studied to generate images of outdoor scenes from semantic label maps coupled with attributes.
\cite{BicycleGANs} presented a framework that is able to model the multi-modal distribution of possible outputs.
%
%
Image-to-image translation has also been well-studied in an unsupervised setting.
\cite{UNIT} studied on unpaired image-to-image translation by training a two-branch GAN. Each branch is composed with a encoder, a generator and a discriminator. With the idea that high-level representation of a pair of corresponding images in two domains should be the same, high-level layers share weights between two branches in encoders, generators and discriminators. 
%
CycleGAN~\cite{CycleGANs}, DiscoGAN~\cite{DiscoGANs} and DualGAN~\cite{DualGANs} developed similar architectures to translate unpaired images which contain, for each, two generators and two discriminators. These methods learn two mappings in an adversarial training process such that an input image in one domain is mapped to a generated image in another, and then the generated image is mapped to a reconstructed image which is closed to the input image in some measures. These methods shared the same idea that since the generated image is able to reconstruct the input image, it should contain the content of the input image. 

Our work focuses on translating face images from corresponding edge maps in a supervised setting, which is able to learn the long-range dependencies and global structure across image.

\subsection{Attention Mechanism}
The convolution operation has a local receptive field. Several layers and large kernel sizes are required to sense the global structure in a large receptive field, which, however, loses the computational and statistical efficiency. Recently, attention mechanisms have been introduced to capture global dependencies \cite{AlignTranslate,ShowAttenTell}.
Self-attention \cite{Non-local,SAGANs} has been shown to be powerful in a variety of tasks.
\cite{Attention} applied self-attention to machine translation models, and demonstrated the plausible effectiveness of self-attention mechanism. 
\cite{Transformer} studied on combining the self-attention mechanism with autoregressive models, and proposed an image transformer model in image generation. 
Inspired by non-local operation in computer vision, \cite{Non-local} utilize self-attention mechanism as a non-local operation to model long-range spatial-temporal dependencies for video processing.
\cite{SAGANs} introduced self-attention to unconditional GANs and achieved state-of-the-art results in generating natural images from noise vectors. Inspired by previous works,  we explore the self-attention mechanisms in the context of image-to-image translation. 
