\section{Related work}
\label{sec:related_work}
Our work is based on image-to-image translation frameworks, which are variants of GANs in a conditional setting. In this section, we present related research in GANs, conditional GANs, and image-to-image translation models. We also give a brief review on recently proposed attention models.
\subsection{Generative Adversarial Networks (GANs)}
Generative adversarial networks (GANs) \cite{GANs} have obtained a great success in recent years.
Based on the minmax game theory, a classical architecture of GANs contains a generator network and a discriminator network. The generator takes a noise vector as input and generates samples indistinguishable from the real ones, while the discriminator, in opposite, attempts to find out whether its input is real or synthesized. The minmax game played by these two networks guides the generated distribution to be similar to the real data distribution. 
Compared to other DNN frameworks of image generation \cite{VAEs, PixelCNN}, GANs are able to synthesize images with less blurriness, and provide a more efficient process to generate samples. However, GANs suffer from several problems in the early stage, such as the instability of training and the mode collapse problem. To stabilize the training process of GANs and enable GANs to generate images with high quality and large diversity, many efforts have been made. 
Deep convolutional GANs (DCGANs) \cite{DCGANs} first introduced a convolutional architecture which led to improved visual quality. 
An approach is proposed to train discriminator in a semi-supervised fashion~\cite{Improved_Techniques}, granting the discriminator's internal representations knowledge of the class structure of (some fraction of) the training data is presented. 
Energy-based GANs (EBGANs) \cite{EBGANs} were proposed as a class of GANs that aim to model the discriminator as an energy function. 
This variant converges more stably and is not only easy to train but also robust to hyper-parameter variations. 
Wasserstein distance, which acts as a loss as well as a measure of convergence in training process, is brought to GANs~\cite{WGANs, WGAN-GP} to benefit both the stability and mode coverage. 
Several other methods~\cite{LSGANs, BEGANs, DRAGANs} also make progress in stabilizing the training and increasing the diversity of the results of GANs. 
A recently proposed technique~\cite{SAGANs} introduced self-attention mechanism to unconditional GANs and achieved state-of-the-art results. 
%
Inspired by this self-attention mechanism, we introduce a conditional self-attention module to capture long-range dependences between parts of faces when transferring lines to realistic images. 
%

\subsection{Conditional Generative Adversarial Networks}

Conditional GANs are generalized versions of GANs in a conditional setting. 
Instead of taking a noise vector as input, conditional GANs generate images based on the assigned conditions and model the conditional distributions of the samples with respect to the conditions. 
%
Conditional GANs were firstly introduced~\cite{cGANs} as treating the conditional generation problem as the inverse processing of image classification and using discrete labels as condition to generate images. 
Previous work has explored GANs for generating images based on a wide variety of conditions.
\cite{InfoGANs, ACGANs} took both noise vectors and discrete class label as input and added a classification task to the discriminator in two different architectures in order to generate images with high recognizability.   
%
\cite{3DGANs} trained convolutional networks to generate images of objects given object style, viewpoint and color. With the experiments of interpolating viewpoints, they showed that networks learn a meaningful representation of 3D models. 
%
\cite{StackGANs, StackGANs++} generated high-resolution photo-realistic images conditioned on text descriptions in two stages, where GANs sketched the basic shapes and colors in the first stage and added details to the generated images in the second stage.  
%
A recently proposed method~\cite{cGAN-projection} leverages the conditional information in a novel way, where the discriminator involves an inner product term between the condition vector and the feature vector in a middle level layer. This formulation is based on the observation that the loss function of GANs can be decomposed into two log likelihood ratios.
%

Our framework utilizes GANs in a conditional setting to generate images from rough lines, and it carries the condition information directly even in the high-level layers.

%
%
\subsection{Image-to-Image Translation with GANs}
% TODO: modify something like [1] studied xxx
Given an image in one domain, image-to-image translation methods generate a corresponding image in another domain. These two images are possible representations of the same scene or object. 
Image-to-image translation with GANs is a special case of conditional GANs where images are applied to be conditions. 
%

The pix2pix method \cite{pix2pix} firstly introduced the concept of image-to-image translation. Pix2pix is trained in a supervised manner, where the training dataset is a set of paired images. 
%Unlike prior works which dealt with only one application, \cite{pix2pix} are able to handle multiple applications in one frame work only switching training datasets. 
To deliver the fine-level structure of the conditional image to the generated image, pix2pix applies skip connections~\cite{Unet} between mirrored layers in the generator to make sure low-level information pass through its encoder-decoder architecture and uses patch discriminators to increase the performance of the generator. 
However, the convolution-based architecture makes it difficult to discover the long-range dependencies across the images and feature maps.
Moreover, the patch-wise discriminator is not able to ensure the global structure information to be well captured by the model.
%
In addition to pix2pix, many image-to-image tasks are trained in a supervised manner. 
\cite{CRN, pix2pixHD} used coarse-to-fine refinement frameworks to synthesis photographic images from semantic label maps. 
\cite{outdoor_scene} studied to generate images of outdoor scenes from semantic label maps coupled with attributes.
\cite{BicycleGANs} presented a framework that is able to model the multi-modal distribution of possible outputs.
%
%
Image-to-image translation has also been well-studied in an unsupervised setting.
\cite{UNIT} studied on unpaired image-to-image translation by training a two-branch GAN. Each branch is composed with a encoder, a generator and a discriminator. With the idea that high-level representation of a pair of corresponding images in two domains should be the same, high-level layers share weights between two branches in encoders, generators and discriminators. 
%
CycleGAN~\cite{CycleGANs}, DiscoGAN~\cite{DiscoGANs} and DualGAN~\cite{DualGANs} developed similar architectures to translate unpaired images which contain, for each, two generators and two discriminators. These methods learn two mappings in an adversarial training process such that an input image in one domain is mapped to a generated image in another, and then the generated image is mapped to a reconstructed image which is closed to the input image in some measures. These methods shared the same idea that the generated image should contain the content of the input image in order reconstruct the input image from it. 
In comparison, our work focuses on translating a rough lines map to a realistic face photo, whose key challenge is to learn the long-range dependencies and global structure across different regions in a face image.

\subsection{Attention Mechanism}
Since the convolution operation has a local receptive field, several layers and large kernel sizes are required to sense the global structure in a large receptive field. 
However, simply combining several convolutional layers loses the computational and statistical efficiency. 
Recently, attention mechanisms have been introduced to capture global dependencies \cite{AlignTranslate,ShowAttenTell}.
Self-attention \cite{Non-local,SAGANs} has been shown to be powerful in a variety of tasks.
\cite{Attention} applied self-attention to machine translation models, and demonstrated the plausible effectiveness of self-attention mechanism. 
\cite{Transformer} studied on combining the self-attention mechanism with autoregressive models, and proposed an image transformer model in image generation. 
Inspired by non-local operations in computer vision, \cite{Non-local} utilized self-attention mechanism as a non-local operation to model long-range spatial-temporal dependencies for video processing.
\cite{SAGANs} introduced self-attention to unconditional GANs and achieved state-of-the-art results in generating natural images from noise vectors. Inspired by previous works, we explore the self-attention mechanisms in the context of image-to-image translation. 
